{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Lecture\n",
    "\n",
    "\n",
    "### Remember our main steps motto _isbe_.\n",
    "1. i - Inspect and explore data.\n",
    "2. s - Select and engineer features.\n",
    "3. b - Build and train model.\n",
    "4. e - Evaluate model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas for data handling\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK is our Natural-Language-Took-Kit\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Libraries for helping us with strings\n",
    "import string\n",
    "# Regular Expression Library\n",
    "import re\n",
    "\n",
    "# Import our text vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Import our classifiers\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Import some ML helper function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "# Import our metrics to evaluate our model\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# You may need to download these from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our data\n",
    "df = pd.read_csv('data/20-newsgroups.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for nulls and dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check out our class balances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.topic_category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "1. Lowercase all words\n",
    "2. Remove all punctuation.\n",
    "3. Remove all stopwords. \n",
    "4. Stem words\n",
    "\n",
    "Remember, you can apply a function to a pandas column by using `df.column_name.apply(your_function)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all words\n",
    "def make_lower(a_string):\n",
    "    return a_string.lower()\n",
    "\n",
    "a_sentence = 'This was A SENTENCE with lower and UPPER CASE.'\n",
    "make_lower(a_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation\n",
    "\n",
    "def remove_punctuation(a_string):    \n",
    "    a_string = re.sub(r'[^\\w\\s]','',a_string)\n",
    "    return a_string\n",
    "\n",
    "\n",
    "a_sentence = 'This is a sentence! 50 With lots of punctuation??? & other #things.'\n",
    "remove_punctuation(a_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What `word_tokenizer` does\n",
    "The `word_tokenizer` helper function accurately splits your data down into words.  \n",
    "You could use just `sentence.split(' ')` to split on spaces, however, that does not take into account new-lines or other edge cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_string = 'This is a sentence!  With lots of punctuation??? & other #things.'\n",
    "words = word_tokenize(a_string)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all stopwords\n",
    "\n",
    "def remove_stopwords(a_string):\n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "    \n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        \n",
    "        # Check if word is not in stopwords\n",
    "        if word not in stopwords:\n",
    "            \n",
    "            # If word not in stopwords, append to our valid_words\n",
    "            valid_words.append(word)\n",
    "\n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string\n",
    "            \n",
    "a_sentence = 'This is a sentence! With some different stopwords i have added in here.'\n",
    "remove_stopwords(a_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break words into their stem words\n",
    "\n",
    "def stem_words(a_string):\n",
    "    # Initalize our Stemmer\n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "\n",
    "    # Loop through all the words\n",
    "    for word in words:\n",
    "        # Stem the word\n",
    "        stemmed_word = porter.stem(word)\n",
    "        \n",
    "        # Append stemmed word to our valid_words\n",
    "        valid_words.append(stemmed_word)\n",
    "        \n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string \n",
    "\n",
    "\n",
    "a_sentence = 'I played and started playing with players and we all love to play with plays'\n",
    "stem_words(a_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize words with pos-tags\n",
    "\n",
    "def convert_pos(pos):\n",
    "    if pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lem_with_pos_tag(a_string):\n",
    "    # Initalize our Lemmer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Break the sentence down into a list of words\n",
    "    words = word_tokenize(a_string)\n",
    "    \n",
    "    # Get the word and pos_tag for each of the words. \n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    \n",
    "    # Make a list to append valid words into\n",
    "    valid_words = []\n",
    "\n",
    "    # Loop through all the words\n",
    "    for word in tagged_words:\n",
    "        \n",
    "        # The word is the first element in the tuple\n",
    "        the_word = word[0]\n",
    "        \n",
    "        # The pos_tag is the second element in the tuple\n",
    "        the_pos_tag = word[1]\n",
    "        \n",
    "        # Convert the pos_tag into the format the lemmatizer accepts\n",
    "        the_pos_tag = convert_pos(the_pos_tag)\n",
    "        \n",
    "        # Lemmatize the word with the pos_tag\n",
    "        lemmed_word = lemmatizer.lemmatize(the_word, the_pos_tag)\n",
    "        \n",
    "        # Append stemmed word to our valid_words\n",
    "        valid_words.append(lemmed_word)\n",
    "        \n",
    "    # Join the list of words together into a string\n",
    "    a_string = ' '.join(valid_words)\n",
    "\n",
    "    return a_string \n",
    "\n",
    "a_sentence = 'I played and started playing with players and we all love to play with plays'\n",
    "another_sentence = 'This is because she wanted to go outside with her friends and play basketball.'\n",
    "lem_with_pos_tag(another_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline(input_string):\n",
    "    input_string = make_lower(input_string)\n",
    "    input_string = remove_punctuation(input_string)\n",
    "    #input_string = lem_with_pos_tag(input_string)\n",
    "    input_string = remove_stopwords(input_string)    \n",
    "    return input_string\n",
    "\n",
    "\n",
    "# df['message_clean'] = df['message']\n",
    "# df['message_clean'] = df['message_clean'].apply(make_lower)\n",
    "# df['message_clean'] = df['message_clean'].apply(remove_punctuation)\n",
    "# df['message_clean'] = df['message_clean'].apply(lem_with_pos_tag)\n",
    "# df['message_clean'] = df['message_clean'].apply(remove_stopwords)\n",
    "# df['message_clean'] = df['message'].apply(text_pipeline)\n",
    "\n",
    "print(\"ORIGINAL TEXT:\", df['message'][0])\n",
    "print(\"CLEANDED TEXT:\", df['message_clean'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our `X` and `y` data. \n",
    "\n",
    "X = df['message_clean'].values\n",
    "\n",
    "y = df['topic_category'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize our vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# This makes your vocab matrix\n",
    "vectorizer.fit(X)\n",
    "\n",
    "# This transforms your documents into vectors.\n",
    "X = vectorizer.transform(X)\n",
    "\n",
    "print(X.shape, type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X is now a 'sparse matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split our data into testing and training like always. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into testing and training like always. \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize our model.\n",
    "model = MultinomialNB(alpha=.05)\n",
    "\n",
    "\n",
    "# Fit our model with our training data.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make new predictions of our testing data. \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Make predicted probabilites of our testing data\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Evaluate our model\n",
    "accuracy =  model.score(X_test, y_test)\n",
    "\n",
    "# Print our evaluation metrics\n",
    "print(\"Model Accuracy: %f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision\n",
    "Out of all the times the MODEL says 'yes' what was the precentage it was correct. \n",
    "* The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative. \n",
    "* The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. \n",
    "* If you want to raise precision (ie; only say yes when you are absolutely sure), raise your classification threshold.\n",
    "\n",
    "# Recall\n",
    "Out of all the times the ACTUAL is 'yes', what percentage did you get correct.  \n",
    "* The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "* The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. \n",
    "* if you want to raise recall, lower your classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of our results\n",
    "fig, ax = plt.subplots(figsize=(21, 21))\n",
    "\n",
    "disp = plot_confusion_matrix(model, X_test, y_test,\n",
    "                             display_labels=model.classes_,\n",
    "                             cmap=plt.cm.Blues, ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see if a Random Forest can do any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "\n",
    "# Fit our model with our training data.\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make new predictions of our testing data. \n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Make predicted probabilites of our testing data\n",
    "y_pred_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Evaluate our model\n",
    "accuracy =  rf_model.score(X_test, y_test)\n",
    "\n",
    "# Print our evaluation metrics\n",
    "print(\"Model Accuracy: %f\" % accuracy)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=rf_model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix of our results\n",
    "fig, ax = plt.subplots(figsize=(21, 21))\n",
    "disp = plot_confusion_matrix(rf_model, X_test, y_test,\n",
    "                             display_labels=rf_model.classes_,\n",
    "                             cmap=plt.cm.Blues, ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "disp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# When classifing a new piece of text.\n",
    "For example, if we wanted to classify a totally new peice of text that was oustide of our training and testing data, what you must first do is feature engineer the new text the same way you did with your training text, and then transform the new text using the same vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = 'I love hockey and scoring goals. The sport of hockey is fun to watch. Wayne Gretzky was the best hockey player of all time.  He scored so many goals.'\n",
    "\n",
    "# Feature engineer the same way we did with our original data. \n",
    "new_text = text_pipeline(new_text)\n",
    "\n",
    "# Sanity check\n",
    "print(new_text)\n",
    "\n",
    "# Turn the new_text into numbers using the vectorizer\n",
    "# NOTE, must be passed in as a list.\n",
    "# NOTE, use just transform here, NOT FIT. \n",
    "new_text_vectorized = vectorizer.transform([new_text])\n",
    "\n",
    "# make a new prediction using our model and vectorized text\n",
    "model.predict(new_text_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the predicted probabilies for each class\n",
    "pp = model.predict_proba(new_text_vectorized)\n",
    "for c, p in zip(model.classes_, pp.round(3)[0]):\n",
    "    print(c, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
